{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75208339",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d96fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yt_dlp\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c880a7",
   "metadata": {},
   "source": [
    "# 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f23e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMによる生成\n",
    "def LLM_gen(contents: str) -> str:\n",
    "    client = genai.Client()\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\", \n",
    "        contents= contents\n",
    "    )\n",
    "    return response.text\n",
    "# 危険な文字の除去\n",
    "def replace_chars(s: str) -> str:\n",
    "    remove_chars = '\\\\/:*?\"<>|￥＜＞｜'  # 削除対象文字\n",
    "    table = str.maketrans({ch: '-' for ch in remove_chars})\n",
    "    s2 = s.translate(table)\n",
    "    s3 = s2.replace(\" \",\"\")\n",
    "    return s3\n",
    "# srtファイルを削除したときにjsonファイルのdoneを全てfalseに修正"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ebdd1",
   "metadata": {},
   "source": [
    "# youtubeの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb88c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=BBe1ij0AyTk', 'done': True, 'title': '【就活考察】東大生の就職志望No1-コンサルティングファーム-歴史と未来', 'LLM_gen': False}\n",
      "文字お越し済み 【就活考察】東大生の就職志望No1-コンサルティングファーム-歴史と未来\n",
      "{'url': 'https://www.youtube.com/watch?v=6GCIwj1X9W4', 'done': True, 'title': 'なぜ鳩のヒナは見かけない？知られざる鳩の生態【アニメ】', 'LLM_gen': False}\n",
      "文字お越し済み なぜ鳩のヒナは見かけない？知られざる鳩の生態【アニメ】\n",
      "{'url': 'https://www.youtube.com/watch?v=OP6SEtY7kO8', 'done': True, 'title': '【世界一流エンジニアのチーム作り】成果を上げるチームとは-人に頼るハードルを下げる-同僚と意見が対立したら？-無駄な会議はどんどん削ろう-「理解」ファーストで取り組もう【米マイクロソフト牛尾剛】', 'LLM_gen': False}\n",
      "文字お越し済み 【世界一流エンジニアのチーム作り】成果を上げるチームとは-人に頼るハードルを下げる-同僚と意見が対立したら？-無駄な会議はどんどん削ろう-「理解」ファーストで取り組もう【米マイクロソフト牛尾剛】\n",
      "{'url': 'https://www.youtube.com/watch?v=KlZ-QmPteqM', 'done': True, 'title': 'GPTとは何かTransformerの視覚化-Chapter5,DeepLearning', 'LLM_gen': False}\n",
      "文字お越し済み GPTとは何かTransformerの視覚化-Chapter5,DeepLearning\n",
      "{'url': 'https://www.youtube.com/watch?v=2Q1rDHtU8Wk', 'done': True, 'title': '【特別編】ハーバードやGoogleで使われている「超効率的学習法」【LIMITLESS超加速学習】', 'LLM_gen': False}\n",
      "文字お越し済み 【特別編】ハーバードやGoogleで使われている「超効率的学習法」【LIMITLESS超加速学習】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsonファイルの読み込み\n",
    "youtube_links_file_name = \"youtube_links.json\"\n",
    "youtube_links_path = Path(youtube_links_file_name)\n",
    "# if not youtube_links_path.exists():\n",
    "#     raise \n",
    "json_text = youtube_links_path.read_text(encoding=\"utf-8\")\n",
    "data = json.loads(json_text)\n",
    "\n",
    "ydl_opts = {\n",
    "    \"skip_download\": True,\n",
    "    \"writesubtitles\": True,\n",
    "    \"writeautomaticsub\": True,\n",
    "    \"subtitleslangs\": [\"ja\"],     # 日本語字幕\n",
    "    \"subtitlesformat\": \"srt\",     # 形式\n",
    "    \"outtmpl\": \"captions/%(title)s.%(ext)s\" \n",
    "}\n",
    "\n",
    "# 文字お越しの読み込み\n",
    "for v in data:\n",
    "    print(v)\n",
    "    url = v['url']\n",
    "    done = v['done']\n",
    "    title = v['title']\n",
    "    \n",
    "    if done and title!=None:\n",
    "        print(f\"文字お越し済み {title}\")\n",
    "        continue\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ret = ydl.download([url])\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        title = info['title']\n",
    "        if ret == 0: # 成功した場合\n",
    "            v['done'] = True\n",
    "            v['title'] = title\n",
    "            print(f\"[OK] {url} {title}\")\n",
    "        else:\n",
    "            print(f\"[FAIL] {url} {title}\")\n",
    "\n",
    "youtube_links_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2842ccd",
   "metadata": {},
   "source": [
    "# 文字お越こし要約(parallel-version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca08037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要約した合計は一致\n",
      "titleが一致 : 【就活考察】東大生の就職志望No1-コンサルティングファーム-歴史と未来 : LLM_gen_flg=False,# コンサルティング業界の現状と人気\n",
      "\n",
      "*   **高偏差値層からの高い人気**\n",
      "    *   東大生・京大生の就職人気ランキングにおいて、上位20社中14社をコンサルティングファームが占める。\n",
      "    *   東大生の約16%がコンサルティング・シンクタンク業界を志望（IT・通信業界の約17%に次ぐ）。\n",
      "    *   一般的な大学生のコンサル志望率（1.4%）と比較して顕著な差があり、高偏差値層に特有の現象。\n",
      "    *   「秀才の集団が論理的思考で問題解決をする」というイメージが先行。\n",
      "\n",
      "# コンサルティングファームの定義と類型\n",
      "\n",
      "*   **定義**\n",
      "    *   企業の業務課題を指摘し、対応策や改善策を提案する会社の総称。\n",
      "    *   企業の依頼を受け、戦略立案や業務改善を行うプロフェッショナル集団。\n",
      "*   **主要な系統**\n",
      "    *   **戦略系：** 事業戦略を主に取り扱い、業界の先駆け。\n",
      "    *   **総合系：** 会計事務所をルーツに持ち、フルラインのサービス（組織、人事、IT戦略など）を提供。\n",
      "    *   **業務IT系：** 高い技術力を誇り、IT化の波に乗って台頭。\n",
      "    *   現在は各社がコンサルティング領域を広げているため、系統による分類の意味は薄れているが、ルーツを知ることは興味深い。\n",
      "*   **日本独自のファーム**\n",
      "    *   シンクタンク系や国内独立系ファームも存在。外資系ファームとは別の流れで発展。\n",
      "\n",
      "# コンサルティング業界の歴史\n",
      "\n",
      "*   **起源（19世紀後半・アメリカ）**\n",
      "    *   フレデリック・テイラーによる「科学的管理法」（生産現場の効率化、労働コスト削減）がコンサルティング業の起源とされる。\n",
      "    *   当初は個人の職業だったが、共同事務所（ファーム）が誕生。世界初のファームは1886年設立のアーサー・D・リトル。\n",
      "    *   初期は事業の効率化を目指すコンサルティングが主流。\n",
      "*   **発展期（第一次世界大戦後〜世界大恐慌）**\n",
      "    *   第一次世界大戦後、「永遠の繁栄」と呼ばれたアメリカ経済の中心化とともに、P.B. & カンパニー、A.T. カーニーなどの戦略系ファームが誕生し急成長。\n",
      "    *   世界大恐慌下でも、景気に左右されず事業提案（好景気時は新規事業、不景気時は改善策）できる強みを発揮し、むしろ発展。\n",
      "    *   ルーズベルト大統領のニューディール政策（政府の経済介入、中立性の重視）により、銀行や弁護士の兼務が制限され、戦略系ファームが業務を委託される機会が増加。\n",
      "    *   個人コンサルから組織コンサルへの移行を促進。\n",
      "*   **IT化の波と業界再編（1950年代〜2000年代）**\n",
      "    *   **コンピューター導入：** 1950年代、会計事務所が企業会計と相性の良いコンピューターを利用したコンサルティングを推進。\n",
      "    *   **本格的なIT化：** 1980年代にIBM PCの登場で企業の業務にコンピューターが導入され、会計事務所系ファームに大型プロジェクトが依頼される。\n",
      "    *   **BPRの提唱：** 1990年代にBPR（業務プロセス再構築）概念が注目され、業務ITコンサルティングが加速。\n",
      "    *   **ドットコムバブル崩壊後：** 会計事務所系ファームは好調を維持。\n",
      "    *   **エンロン事件とソックス法：** 2001年のエンロン事件（不正会計）を契機に、米国で企業の不祥事に対する罰則を強化するソックス法が制定。\n",
      "        *   会計監査を行う会計事務所のコンサルティング業務が禁止され、コンサルティング部門の独立・分離が進み、業界再編に発展。\n",
      "\n",
      "# 日本におけるコンサルティング業界の発展\n",
      "\n",
      "*   **黎明期**\n",
      "    *   20世紀半ばから日本能率協会のような独立系ファームが活動。\n",
      "    *   1960年代からボストン コンサルティング グループなど主要な外資系ファームが日本オフィスを開設したが、当時はアウトソーシングの文化が未熟で活動は限定的。\n",
      "*   **成長と定着**\n",
      "    *   1980年代に入りコンサルティング文化が根付き、野村総合研究所、日本総合研究所などが台頭。\n",
      "    *   バブル崩壊後の1990年代前半は一時苦難に直面するも、後半には大企業が中長期戦略計画、組織改革、グローバル展開などを依頼するようになり、業界は再活性化。\n",
      "    *   現在、日本のトップ企業のほとんどがファームを利用。外資系、国内独立系など選択肢が広く、テーマによる棲み分けが定着し、共存関係にある。\n",
      "\n",
      "# 現代のトレンドと未来の展望\n",
      "\n",
      "*   **市場規模の拡大**\n",
      "    *   2020年パンデミック後もDX（デジタルトランスフォーメーション）需要に牽引され、日本国内市場は2024年には1兆円を超える規模に到達すると予測。\n",
      "    *   多くの外資系ファームが1兆円以上の売上を上げ、その影響力の高さを示す。\n",
      "*   **デジタル領域の強化**\n",
      "    *   データ系ファームの買収や、戦略コンサルティングファームのデジタル分野への参入が顕著。\n",
      "    *   マッキンゼー、ボストン コンサルティング グループなどもデジタルを強化すべく新会社を傘下に設立。\n",
      "    *   アクセンチュアは2020年に組織を再編し、デジタル領域の人材を他のすべての部門に組み込み、「デジタルをどう実現するか」という視点に注力。顧客要望に応じた迅速な実装環境を整備。\n",
      "    *   戦略系コンサルティングにおいてもデジタル領域の知見が必須に。\n",
      "*   **デザイン領域への進出と広告業界への影響**\n",
      "    *   多くのコンサルティングファームがデザイン会社を傘下に収める動きが活発化。\n",
      "    *   総合系コンサル会社が広告会社の存在を脅かし、デジタル広告市場の収益ランキング上位にコンサル会社の名前が連なるように。\n",
      "    *   きっかけは広告マーケティングプロジェクト、特に「アクチュアル保証」（実際の視聴率に基づく広告費交渉）支援。\n",
      "    *   アクセンチュアが広告クリエイティブ会社を買収するなど、マーケティングコンサル領域のみならず、広告クリエイティブやメディアバイイングといった広告会社の全領域への進出も懸念される。\n",
      "*   **今後の人材像と業界の方向性**\n",
      "    *   単なる経営コンサルティングに留まらず、IT、会計、業務改善のプロ集団としての利点を生かし、新たな領域へ柔軟かつ迅速に参入していくと予想。\n",
      "    *   高偏差値に裏付けられた「論理的思考」と、デザイン会社買収によって取り込む「豊かな感性のデザイン思考」を融合させることで、業界がどのように進化するかが注目される。\n",
      "要約を出力します : 【就活考察】東大生の就職志望No1-コンサルティングファーム-歴史と未来\n",
      "【就活考察】東大生の就職志望No1-コンサルティングファーム-歴史と未来.md\n",
      "titleが一致 : なぜ鳩のヒナは見かけない？知られざる鳩の生態【アニメ】 : LLM_gen_flg=False,# 鳩駆除作戦の顛末\n",
      "\n",
      "## 上司からの特命\n",
      "*   出勤途中に鳩に糞をかけられた上司が激怒し、鳩のひな殲滅を命令。\n",
      "*   ペンギンとパンダが鳩のひな駆除を命じられる。\n",
      "\n",
      "## 鳩のひな探しと潜入作戦\n",
      "*   鳩のひなや巣は、天敵や人間から隠れるため、建物の隙間や高架下などの入り組んだ場所に作られ、見つけにくい。\n",
      "*   ペンギンとパンダは鳩に変装して群れに潜入。\n",
      "*   室外機の裏に鳩の巣とひなを発見する。\n",
      "*   **鳩の生態の発見**:\n",
      "    *   ひなは成長初期は黄色く、猫から身を守るため巣の中で育てられる。\n",
      "    *   親鳩は「ハトミルク」と呼ばれる高栄養なミルクを生成し、ひなに口移しで与える（オスメス両方が生成可能）。\n",
      "    *   ハトミルクの栄養価が高いため、ひなは20日程度で急速に成長し、大人になる。\n",
      "\n",
      "## 心境の変化と作戦中止\n",
      "*   ひなの成長と親子の情愛を目にし、ペンギンとパンダはひなを駆除することにためらいを感じる。\n",
      "*   上司もひなの可愛さに触れ、鳩との共存を認め、駆除作戦は中止となる。\n",
      "\n",
      "## 鳩の隠れた危険性\n",
      "*   ペンギンたちが体調不良を訴え、鳩がオウム病やサルモネラ食中毒など様々な病原菌の塊であることが判明。\n",
      "*   上司は再び鳩嫌いとなり、「鳩には近づきすぎないように」と注意を促す。\n",
      "要約を出力します : なぜ鳩のヒナは見かけない？知られざる鳩の生態【アニメ】\n",
      "なぜ鳩のヒナは見かけない？知られざる鳩の生態【アニメ】.md\n",
      "titleが一致 : 【世界一流エンジニアのチーム作り】成果を上げるチームとは-人に頼るハードルを下げる-同僚と意見が対立したら？-無駄な会議はどんどん削ろう-「理解」ファーストで取り組もう【米マイクロソフト牛尾剛】 : LLM_gen_flg=False,# 生産性の高いチーム文化の考察\n",
      "\n",
      "## 他者との協調と依頼\n",
      "*   **日本とアメリカの文化比較**\n",
      "    *   日本人は他者に頼ることが苦手だが、アメリカ人は容易に頼り、容易に断る文化を持つ。\n",
      "    *   これによりストレスが少なく、生産性が向上する。\n",
      "*   **「ピング」の活用と知識共有**\n",
      "    *   自分の手に負えない問題は、すぐに専門知識を持つ同僚に「ピング」（連絡）して協力を求める。\n",
      "    *   新入社員であっても、タスク全体を任され、困った際は他者の脳を借りて解決する。\n",
      "    *   互いに教え合う文化があり、誰も質問を馬鹿にしない（スティーブ・ジョブズの例）。\n",
      "*   **容易な依頼と拒否のセット**\n",
      "    *   助けを求める側が簡単に依頼できるのは、助ける側も簡単に断れるという文化とセット。\n",
      "    *   忙しい、わからない場合は気軽に断り、別の解決策（他の人を紹介するなど）を提示することも多い。\n",
      "    *   これにより、過度な負担やストレスを防ぐ。\n",
      "\n",
      "## 意見対立時の対応\n",
      "*   **敵対的ではないフィードバック**\n",
      "    *   意見の相違があっても、敵対的な関係にはならない。\n",
      "    *   「フィードバックを得る場」として捉えられ、建設的な議論が行われる。\n",
      "*   **感情を排したロジカルな議論**\n",
      "    *   感情が乗ることが少なく、単にロジカルに意見を述べ合う。\n",
      "    *   「相手のアイデアを潰す」という意図はなく、より良い解決策を探る。\n",
      "*   **最終決定権はタスク担当者**\n",
      "    *   様々な意見が出たとしても、最終的な決定権はそのタスクを任されている担当者にある。\n",
      "    *   これにより、スムーズな意思決定と進行が可能となる。\n",
      "\n",
      "# 生産性の高いチームと低いチームの違い\n",
      "\n",
      "## 生産性の低いチームの特徴\n",
      "*   **無駄な作業の多さ**\n",
      "    *   「これって意味あるかな？」と思いながらも継続している作業が多い。\n",
      "    *   形式的な承認プロセス、何ヶ月もかける設計書作成など、価値のない無駄な工程が多い。\n",
      "*   **変化への抵抗**\n",
      "    *   既存のやり方を変えず、無駄なプロセスが温存されがち。\n",
      "\n",
      "## 生産性の高いチームの特徴\n",
      "*   **無駄の徹底的な排除**\n",
      "    *   無駄な会議や非効率なステップを常に洗い出し、積極的に排除する。\n",
      "*   **フィードバックと変化への適応**\n",
      "    *   実施したことに対しフィードバックを得て、「無駄だ」と判断したらすぐに改善策を講じる。\n",
      "    *   世間の動向や新しいツール（AIなど）を積極的に取り入れ、変化し続ける。\n",
      "    *   地道な改善の積み重ねが、長期的な生産性向上につながる。\n",
      "\n",
      "# ビジネスパーソンが習得すべきエッセンス\n",
      "\n",
      "*   **「理解」への投資**\n",
      "    *   生産性にこだわるなら、タスクや問題に対する「理解」に時間を費やすことが最も重要。\n",
      "    *   「走りながら考えろ」ではなく、最初に立ち止まってしっかり理解することで、その後の作業スピードと質が飛躍的に向上する。\n",
      "要約を出力します : 【世界一流エンジニアのチーム作り】成果を上げるチームとは-人に頼るハードルを下げる-同僚と意見が対立したら？-無駄な会議はどんどん削ろう-「理解」ファーストで取り組もう【米マイクロソフト牛尾剛】\n",
      "【世界一流エンジニアのチーム作り】成果を上げるチームとは-人に頼るハードルを下げる-同僚と意見が対立したら？-無駄な会議はどんどん削ろう-「理解」ファーストで取り組もう【米マイクロソフト牛尾剛】.md\n",
      "titleが一致 : GPTとは何かTransformerの視覚化-Chapter5,DeepLearning : LLM_gen_flg=False,# GPTとトランスフォーマーの概要\n",
      "\n",
      "### GPTとは\n",
      "*   **G**enerative **P**re-trained **T**ransformer の略。\n",
      "*   新しいテキストを生成するボット。\n",
      "*   「事前訓練」は大量のデータで学習済みであることを意味し、特定のタスク向けの追加トレーニング（ファインチューニング）の余地がある。\n",
      "*   「トランスフォーマー」は、現在のAIブームの中心的な発明である特定の種類のニューラルネットワーク。\n",
      "\n",
      "### トランスフォーマーの応用\n",
      "*   原型は2017年にGoogleが言語翻訳のために発明。\n",
      "*   ChatGPTなどの基盤モデルは、テキスト（および場合によっては画像や音声）を受け取り、次に続くものを予測するよう訓練されている。\n",
      "*   その他の応用例：音声認識、テキストからの合成音声生成、DALL-EやMidjourneyのようなテキストからの画像生成AI。\n",
      "\n",
      "# テキスト生成の仕組み\n",
      "\n",
      "### 予測とサンプリング\n",
      "*   モデルは次に続くテキストの候補（単語や塊）の確率分布を予測する。\n",
      "*   この予測モデルを利用し、生成された確率分布からランダムなサンプルを選び、それをテキストに連結して、再度全体について予測プロセスを繰り返すことで、長い文章を生成できる。\n",
      "*   GPT-2のような小規模モデルでは意味をなさない場合もあるが、GPT-3のような大規模モデルでは意味のあるストーリーが生成される。ChatGPTなどの大規模言語モデルもこの「繰り返しの予測とサンプリング」で動作する。\n",
      "\n",
      "# トランスフォーマーのデータ処理フロー\n",
      "\n",
      "### 概要\n",
      "チャットボットが単語を生成する際のデータ処理は、以下のステップで構成される。\n",
      "\n",
      "1.  **入力のトークン化**:\n",
      "    *   入力（テキスト、画像、音声など）を「トークン」と呼ばれる小さな部分に分割する。\n",
      "    *   テキストの場合、単語、単語の一部、一般的な文字の組み合わせなどがトークンとなる。\n",
      "2.  **トークンのベクトル化（埋め込み）**:\n",
      "    *   各トークンは「ベクトル」（数のリスト）と結びつけられ、そのトークンの意味を表現する。\n",
      "    *   似た意味の単語は、この高次元空間内で互いに近いベクトルとなる傾向がある。\n",
      "3.  **アテンションブロック**:\n",
      "    *   ベクトル同士が情報を渡し合い、値を更新する演算が行われる。\n",
      "    *   文脈に応じて単語の意味が異なることを考慮し、どの単語が他の単語と関連しているかを把握し、意味（ベクトル）を更新する。\n",
      "4.  **多層パーセプトロン（フィードフォワード層）**:\n",
      "    *   各ベクトルが互いに干渉することなく、同じ演算を並行して行う。\n",
      "    *   それぞれのベクトルに対して複数の質問をし、その答えに基づいて値を更新するようなもの。\n",
      "5.  **プロセスの繰り返し**:\n",
      "    *   アテンションブロックと多層パーセプトロンブロックを繰り返し、最終的に入力文章全体の基本的な意味が最後のベクトルに集約される。\n",
      "6.  **出力の生成**:\n",
      "    *   最後のベクトルに特定の演算を施し、次に来る可能性のある全トークンの確率分布を作成する。\n",
      "\n",
      "### チャットボットへの応用\n",
      "*   ユーザーとAIアシスタントの対話設定（システムプロンプト）を作成し、ユーザーの最初の質問からAIアシスタントが応答しそうな内容を予測させることでチャットボットとして機能する。\n",
      "\n",
      "# 深層学習の背景\n",
      "\n",
      "### 機械学習の考え方\n",
      "*   データを用いてモデルの振る舞いを決定するアプローチ。\n",
      "*   具体的な手続きをコードで定義する代わりに、調整可能な多数のパラメーター（ダイヤルやノブ）を持つ柔軟な構造を用意する。\n",
      "*   入力に対する適切な出力例を大量に与え、パラメーターを調整することで、その振る舞いを模倣させる。\n",
      "*   例：線形回帰。GPT-3は1750億のパラメーターを持つ。\n",
      "\n",
      "### 深層学習モデルの形式\n",
      "*   大規模モデルでも学習アルゴリズム（逆電波）がうまく機能するために、特定の形式に従う必要がある。\n",
      "    *   **入力**: 実数の配列（リスト、多次元テンソル）。\n",
      "    *   **層**: 入力データは多数の異なる層で変換される。\n",
      "    *   **出力**: 最後の層が最終的な出力を表す実数の配列（例：次のトークンの確率分布）。\n",
      "    *   **パラメーター**: 主に「重み」と呼ばれ、データ処理における重み付き和に関わる。計算は行列とベクトルの積で表現されることが多い。\n",
      "\n",
      "# GPT-3における具体的な処理ステップ\n",
      "\n",
      "### 1. トークン化と埋め込み行列 (`We`)\n",
      "*   **トークン**: 入力テキストは単語、単語の一部、記号などのトークンに分割される。\n",
      "*   **埋め込み行列**: モデルの定義済みの語彙（例：50,257トークン）の各トークンに対応する列を持つ行列。\n",
      "    *   これらの列が、各トークンを初期の埋め込みベクトルに変換する。\n",
      "    *   パラメーターであり、データに基づいて学習される（ランダムな初期値から）。\n",
      "*   **埋め込みベクトル**: 高次元空間内の点を指し、トークンの意味を表現する。\n",
      "    *   GPT-3では12288次元。\n",
      "    *   学習を通じて、空間内の方向が言葉の意味と関連するようになる（例：「女性」と「男性」の差ベクトルが「女王」と「王」の差ベクトルに類似する）。\n",
      "    *   **内積**: 2つのベクトルがどれだけ揃っているか（類似性）を測る方法として機能する。\n",
      "    *   **パラメーター数**: GPT-3では約6億1700万。\n",
      "\n",
      "### 2. コンテキストサイズと文脈の吸収\n",
      "*   埋め込みベクトルは、単語単体だけでなく、周囲の文脈情報も吸収する余裕を持つ。\n",
      "*   ネットワークは、各ベクトルがより豊かで具体的な意味を吸収できるようにする。\n",
      "*   **コンテキストサイズ**: ネットワークが一度に処理できるベクトルの固定数。\n",
      "    *   GPT-3では2048。\n",
      "    *   トランスフォーマーが次の単語を予測する際に扱えるテキストの量を決定するため、長い会話ではチャットボットが文脈を失う原因となることがある。\n",
      "\n",
      "### 3. 最終出力層と掘り出し行列 (`WU`)\n",
      "*   最終的に欲しい出力は、次に来るトークンの確率分布。\n",
      "*   **掘り出し行列**: ネットワークの最後のベクトルから、語彙にある各トークンに対応する値を導き出すための行列。\n",
      "    *   これもパラメーターであり、トレーニングによって学習される。\n",
      "    *   **パラメーター数**: GPT-3では約6億1700万が追加され、合計約12億パラメーター。\n",
      "*   **ソフトマックス関数**: 掘り出し行列からの未正規化の出力（ロジット）を有効な確率分布に変換する標準的な方法。\n",
      "    *   各値が0から1の間に収まり、合計が1になるように正規化する。\n",
      "    *   大きな入力値は1に近く、小さな入力値は0に近くなる。\n",
      "    *   **温度 (T) パラメーター**: 分布のシャープさを調整する。\n",
      "        *   Tが大きいほど分布は均一に近くなり、多様な単語が選ばれやすくなる（創造性が増すが、意味不明になるリスクも）。\n",
      "        *   Tが小さいほど大きな値が分布を独占し、最も確からしい単語が選ばれる傾向が強くなる。T=0では常に最大値が選ばれる。\n",
      "\n",
      "# 今後の展望\n",
      "*   単語埋め込み、ソフトマックス、内積、行列演算の理解が、次回解説されるトランスフォーマーの心臓部であるアテンション機構の理解の基礎となる。\n",
      "要約を出力します : GPTとは何かTransformerの視覚化-Chapter5,DeepLearning\n",
      "GPTとは何かTransformerの視覚化-Chapter5,DeepLearning.md\n",
      "titleが一致 : 【特別編】ハーバードやGoogleで使われている「超効率的学習法」【LIMITLESS超加速学習】 : LLM_gen_flg=False,## 脳の可能性を最大限に引き出す\n",
      "\n",
      "### 限界を超えるための3つのリミット（書籍「リミットレス」より）\n",
      "\n",
      "人間の能力や可能性には限界があるという思い込みを打ち破り、成長を志すすべての人がリミットを超えて進歩できる。そのためには、以下の3つのリミットを外す必要がある。\n",
      "\n",
      "*   **マインドセットのリミット**：自分の能力や可能性への低い期待を打ち破る。ネガティブな固定観念を払拭する。\n",
      "*   **モチベーションのリミット**：意欲や目的が不足し行動できない状態を改善する。\n",
      "*   **メソッドのリミット**：成果を得るための効果的な手段を知らない、または実践できていない状態を改善する。\n",
      "\n",
      "### モチベーションを高める3つのポイント\n",
      "\n",
      "1.  **心に火をつける目的を見つける**\n",
      "    *   自分の価値観を明確にする。\n",
      "    *   積極的に新しい経験をすることで情熱を発見する。\n",
      "2.  **脳の燃料補給とメンテナンスを行う**\n",
      "    *   **燃料補給（ブレインフード10選）**：水、アボカド、ブロッコリー、緑の葉物野菜、ブルーベリー、卵、魚介類（サーモン、イワシ、キャビア）、ダークチョコレート、くるみ、ターメリックを積極的に摂取する。\n",
      "    *   **メンテナンス**：十分な質と量の睡眠、適度な運動、ストレス解消、綺麗な環境、良好な人間関係を整える。\n",
      "3.  **習慣を味方につける**\n",
      "    *   目標達成のために習慣を最適化する。\n",
      "    *   **著者の朝のルーティン例**：ベッドを整える、水分補給、利き手と逆の手で歯磨き、運動、冷水シャワー、呼吸エクササイズ、瞑想、ブレインティー、日記、目標設定、読書、スムージー。\n",
      "    *   朝一番でベッドを整えるなど、小さな達成感を得ることでドーパミンやテストステロンが分泌され、良い一日をスタートできる。\n",
      "\n",
      "### 集中力を高めるメソッド\n",
      "\n",
      "脳に最適な学び方を実践する。\n",
      "\n",
      "1.  **シングルタスクの実践**\n",
      "    *   目の前の1つのことに集中する。同時並行作業を避け、時間を区切って一つの作業に没頭する。\n",
      "    *   GTD（Getting Things Done）のように、タスクを小さく区切り、正しい優先順位で1つずつ確実に終わらせることで、ストレスを減らし効率を高める。\n",
      "2.  **フロー状態への移行**\n",
      "    *   目の前の行為に完全にのめり込む状態（ゾーン）に入るための5つのコツ。\n",
      "        *   注意散漫の原因をとことん排除する（シングルタスク、作業環境を整える）。\n",
      "        *   十分な時間を確保する（最低90分、できれば2時間）。\n",
      "        *   好きなことをする。\n",
      "        *   目標を明確にする。\n",
      "        *   少しだけハードルを高く設定する（簡単すぎず、難しすぎない）。\n",
      "\n",
      "## 運動による脳機能向上（書籍「一流の頭脳」より）\n",
      "\n",
      "脳機能を上げ、頭を良くするためには運動が最も有効である。\n",
      "\n",
      "### 運動が脳にもたらす変化\n",
      "\n",
      "運動は体全体と脳の血流を増加させ、脳内で新しい細胞や血管の形成、領域同士の結合強化を促し、脳機能を向上させる。\n",
      "\n",
      "*   **短期的なメリット**：運動直後にドーパミン（やる気ホルモン）の分泌量が増え、数時間その状態が続く。負荷が大きい運動ほど効果が高い。\n",
      "*   **長期的なメリット**：習慣的な運動（週2～3回、半年以上）により脳の**前頭前野**が発達する。BDNF（脳由来神経栄養因子）が分泌され、脳細胞の成長・機能維持が促進される。\n",
      "    *   **前頭前野**：やる気、集中力、記憶力、論理的思考、客観的思考、行動・感情のコントロール、コミュニケーションなどを司る、脳の最高中枢。\n",
      "*   **アイデア出しへの応用**：スティーブ・ジョブズやシリコンバレーのエリートたちがウォーキングミーティングを取り入れるなど、運動はクリエイティブな思考にも有効。\n",
      "\n",
      "### やる気を高める脳内物質\n",
      "\n",
      "運動は以下の3つの脳内物質の分泌を促し、モチベーションを高める。\n",
      "\n",
      "*   **セロトニン（幸せホルモン）**：精神の安定、心の落ち着き、安らぎに関わる。\n",
      "    *   **分泌促進方法**：日光を浴びる（特に午前中）、単調リズム運動（散歩、ラジオ体操、ガムを噛む）、咀嚼。\n",
      "*   **ノルアドレナリン（やる気、集中力、注意深さ）**：適度なストレス下で分泌され、最高のパフォーマンスを引き出す（ヤーキーズ・ドットソンの法則）。\n",
      "    *   **分泌促進方法**：タイムプレッシャー（締め切り効果）。\n",
      "*   **ドーパミン（やる気、活力、快楽、記憶力）**：\n",
      "    *   **分泌促進方法**：小さな達成感を得る。\n",
      "\n",
      "### 効果的な運動方法\n",
      "\n",
      "*   **最低ライン**：1日30分のウォーキング。\n",
      "*   **最高のコンディション**：週に3回、45分以上のランニング。心拍数を上げることが重要。\n",
      "*   **運動の種類**：\n",
      "    *   **有酸素運動**（早歩き、ランニング、サイクリング、スイミング）：脳機能を高める効果が高い。\n",
      "    *   **無酸素運動**（筋トレ、ダッシュ）：筋力向上やテストステロン分泌などのメリットがある。\n",
      "    *   理想は有酸素運動と無酸素運動を組み合わせ、自分のペースで徐々にステップアップすること。\n",
      "\n",
      "## 地頭を鍛える8つの方法（書籍「眠れなくなるほど面白い脳の話」より）\n",
      "\n",
      "「地頭がいい」とは、その人本来の頭の良さであり、論理的思考力やコミュニケーション能力などを指す。これは脳の**前頭前野**を鍛えることで向上する。\n",
      "\n",
      "### 前頭前野の役割\n",
      "\n",
      "*   やる気、集中力、記憶力、論理的思考、客観的思考、行動や感情のコントロール、コミュニケーション。\n",
      "*   発達している人は人付き合いがうまく、長期的に有利な選択ができ、社会的・経済的地位が高くなる傾向がある。\n",
      "\n",
      "### 実践的な鍛え方\n",
      "\n",
      "1.  **雑音を利用する**：あえて雑音のある環境で作業や勉強することで、集中力を鍛え、前頭前野を発達させる。\n",
      "2.  **タイムプレッシャー**：あらかじめ時間を決めて作業を終えるチャレンジをすることで、脳に負荷をかけ鍛える。\n",
      "3.  **マインドフルネス**：瞑想などを通じて「気づき」を高め、前頭前野を鍛える。\n",
      "4.  **色々な体験をする**：スポーツ、旅、美術、映画、読書、散歩など、脳の多様な部分に刺激を与える。特に結果がある程度予想できるが完全には予測できない「具有性」のある体験（旅行、人との会話、ボードゲーム、スポーツ）が有効。\n",
      "5.  **脳に良い栄養を取る**：以下の栄養素を意識的に摂取する。\n",
      "    *   **DHA**：脳の発達・活性化、記憶力・集中力向上（イワシ、サバ、アボカド）。\n",
      "    *   **必須アミノ酸**：\n",
      "        *   チロシン（ドーパミンの原料）：アーモンド、バナナ、牛肉、鶏肉、コーヒー、卵など。\n",
      "        *   トリプトファン（セロトニンの原料）：豚肉、牛肉、豆腐、納豆、乳製品など。\n",
      "    *   **ポリフェノール**：記憶力・思考力向上、抗酸化作用（チョコレート、大豆、緑茶、赤ワインなど）。\n",
      "    *   **ビタミンB6**：脳のエネルギー吸収、脳内物質生産補助（お米、ジャガイモ、肉、卵、魚介類、ナッツ類など）。\n",
      "    *   栄養価の高い食品（全粒穀物、野菜、果物、魚、鶏肉、豆類、ナッツ類、オリーブオイル、コーヒー、お茶など）を増やし、お菓子、加工肉、揚げ物、ファストフードを減らす。\n",
      "6.  **心理的安全状態を作る**：危険、恐怖、不安、未知、曖昧が少ない状態。生活の安定、貯金、生活リズム、良好な人間関係、経験、スキル、ぶれない価値観などが寄与する。\n",
      "7.  **ノーシーボ効果を排除する**：悪い思い込み（「どうせ無理」など）が脳のパフォーマンスを下げるため、自分の良い面や強みを知り、目標ハードルを適正にし、周りを気にせず自分の軸で生きることで排除する。\n",
      "8.  **ア体験（アハ・エクスペリエンス）**：「なるほど分かった」「思い出した」という瞬間に脳の回路（特にクリエイティブに関わる回路）が強化される。すぐに検索せず、自力で思い出そうとする努力が脳を成長させる。\n",
      "\n",
      "### AI時代に必要な能力\n",
      "\n",
      "AI時代に重宝されるのは、人間らしさ（コミュニケーション能力、想像力、直感力、センス、発明、発想、身体性）である。これらは前頭前野を鍛えることで向上し、時代の変化に対応する能力も高まるため、前頭前野を鍛えることは最高の自己投資となる。\n",
      "\n",
      "## 科学的根拠に基づく最高の勉強法（書籍「科学的根拠に基づく最高の勉強法」より）\n",
      "\n",
      "### 1. アクティブリコール\n",
      "\n",
      "*   **概要**：勉強したことや覚えたいことを能動的に思い出す、記憶から引き出す学習法。インプット量よりもアウトプットが記憶の定着に重要。\n",
      "*   **実践法**：\n",
      "    *   練習問題や過去問を解く。\n",
      "    *   暗記カードやフラッシュカードを使う。\n",
      "    *   紙に書き出す。\n",
      "    *   学んだことを誰かに教える（プロテジー効果）。\n",
      "    *   **特に効果的なのは**：ヒントが少ない状態で記憶から引き出すこと（例：複数選択肢より書き出し問題、空欄補充より白紙に書き出す）。\n",
      "*   **著者の白紙勉強法**：\n",
      "    1.  覚えたい情報を読み込む。\n",
      "    2.  情報を見ずに、白い紙に内容をできるだけ書き出す。覚えにくい場合は声に出しながら（プロダクション効果）、誰かに教えるように説明しながら行う。\n",
      "    3.  書き出し終えたら、分からなかった点や忘れている点を教科書などで確認する。\n",
      "    4.  満足するまで書き出しと確認を繰り返す。\n",
      "\n",
      "### 2. 分散学習\n",
      "\n",
      "*   **概要**：一度にまとめて勉強する集中学習（一夜漬け）よりも、時間を開けて繰り返し学習する方が長期的な記憶の定着に良い。脳は生きる上で不要な情報を忘れるようにできているため、繰り返し思い出すことで「必要な情報」だと脳に伝える。\n",
      "*   **最強の勉強法は連続的再学習**：アクティブリコールと感覚反復を組み合わせる。\n",
      "    1.  新しい範囲を勉強する際、少なくとも1〜3回は内容を思い出せるまでアクティブリコールを行う。\n",
      "    2.  その後、1日〜1週間後に再度アクティブリコールを行う。\n",
      "    3.  忘れている内容があれば知識を確認し、再度アクティブリコールを行う。これを何度か繰り返す。\n",
      "\n",
      "### 3. インターリービング\n",
      "\n",
      "*   **概要**：似ているけれども異なった複数のスキルや勉強のトピックを交互に学習する方法。\n",
      "*   **効果**：\n",
      "    *   知識の学習だけでなく、ゴルフのストロークやバスケットボールのパス、音楽の練習など運動スキルにも応用できる。\n",
      "    *   特定の科目をまとめて学ぶブロック学習と異なり、インターリービングでは問題ごとにどの概念や解法を適用すべきか自分で考える必要があり、負荷が高い分、より深い理解と本番での対応力を養う。\n",
      "要約を出力します : 【特別編】ハーバードやGoogleで使われている「超効率的学習法」【LIMITLESS超加速学習】\n",
      "【特別編】ハーバードやGoogleで使われている「超効率的学習法」【LIMITLESS超加速学習】.md\n"
     ]
    }
   ],
   "source": [
    "# ファイル出力なしで要約成功したバージョン\n",
    "import asyncio \n",
    "title_list = []\n",
    "contents_dic = {} \n",
    "\n",
    "# パス設定\n",
    "caption_dir= Path('captions')\n",
    "summary_dir = Path('summary')\n",
    "summary_dir.mkdir(exist_ok=True)\n",
    "# gemini api keyの読み込み\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# 正規表現を使いtextの抽出とLLMによる要約\n",
    "text_pattern = r\"\\d\\n.*\\n(.*)\\n\"\n",
    "for p in caption_dir.iterdir(): \n",
    "    p_str = str(p) \n",
    "    title_pattern = r\"captions\\\\(.*).ja.srt\" \n",
    "    title = re.search(title_pattern, p_str)[1] # タイトル部分の抽出 \n",
    "    replaced_title = replace_chars(title) # 危険文字の変換 \n",
    "    text = p.read_text(encoding='utf-8') \n",
    "    matchs = re.findall(text_pattern, text) \n",
    "    text = '\\n'.join(matchs) \n",
    "    contents= ( \n",
    "        \"以下はyoutube動画の文字お越しをした文章です。\" \n",
    "        \"要約してください。\" \n",
    "        \"ただし、出力はMarkdown形式のみで行い、\" \n",
    "        \"要約に関係ない説明文や前置きは一切書かないでください。\" \n",
    "        \"見出し・箇条書きを適宜使って整理してください。\\n\\n\" \n",
    "        + text \n",
    "        ) \n",
    "    title_list.append(replaced_title)\n",
    "    contents_dic[title] = contents\n",
    "    \n",
    "async def llm_task(contents: str): \n",
    "    loop = asyncio.get_running_loop() \n",
    "    return await loop.run_in_executor(None, LLM_gen, contents) \n",
    "async def main(): \n",
    "    tasks = [llm_task(contents_dic[title]) for title in contents_dic] \n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    json_text = youtube_links_path.read_text(encoding=\"utf-8\")\n",
    "    data = json.loads(json_text)\n",
    "    if len(data) == len(results):\n",
    "        print(f\"要約した合計は一致\")\n",
    "    else:\n",
    "        print(f\"要約失敗が存在\")\n",
    "        raise RuntimeError\n",
    "\n",
    "    # youtube_links.jsonに関するループ\n",
    "    for v in data:\n",
    "        LLM_gen_flg = False\n",
    "        name_match_flg = False\n",
    "        title = v['title']\n",
    "        # title_listsに関するループ\n",
    "        for t in title_list:\n",
    "            if t == v['title']:\n",
    "                LLM_gen_flg = v['LLM_gen']\n",
    "                name_match_flg = True\n",
    "                i = title_list.index(title)\n",
    "                res = results[i]\n",
    "                print(f\"titleが一致 : {title } : {LLM_gen_flg=},{res}\")\n",
    "                break\n",
    "            \n",
    "                v['LLM_gen'] = True\n",
    "            \n",
    "        if not name_match_flg:\n",
    "            print(f\"titleが不一致 : {title}\")\n",
    "            break\n",
    "\n",
    "        if LLM_gen_flg==False:\n",
    "            print(f\"要約を出力します : {title}\")\n",
    "            file_name = summary_dir / f\"{title}.md\"\n",
    "            print(file_name.name)\n",
    "            file_name.write_text(res,encoding='utf-8')\n",
    "            v['LLM_gen'] = True\n",
    "        else:\n",
    "            print(f\"要約出力済みなのでスキップ :  {title}\")\n",
    "    youtube_links_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-summarize-quiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
